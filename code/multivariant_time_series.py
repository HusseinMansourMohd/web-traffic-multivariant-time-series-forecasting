# -*- coding: utf-8 -*-
"""multivariant time series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ApbwlS-BRD4leJ-naC_ApNT-ippDw_Ub
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import keras

#upload csv data-set
train_1 = pd.read_csv('/content/drive/MyDrive/data/train_1.csv')

train_1.head()

#remove the namses of pases only keeping  the indexs
train_1 = train_1.fillna(0)
train_1 = train_1.drop('Page', axis = 1)
train_1.head()

train_1.shape

num=1000
dataset =train_1.iloc[:num, :].values  #iloc all the data values  in coulmns for the first 10 rows         #change the data set to numpy array
dataset_10 = np.array(dataset)
dataset = np.array(dataset)           #change the data set to numpy array
#dataset_10 =dataset_10 /100              # divide each by 100 because ofthe hyperprameters are also factors
dataset_10 =dataset_10.transpose((1, 0))
dataset =dataset.transpose((1, 0))   # transpose the dataset from 10:550 to 550:10 
dataset_10.shape                          # (10, 550)

#transfer the dataset from np_array to a tensor_slices
window_size = 90#60      #number of previous timesteps  used to predict the label 
batch_size = 100        #number of series pasted to the neural net per step
shuffle_buffer_size = 1000    #shuffle the data for better training
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  dataset = tf.data.Dataset.from_tensor_slices(series)           
  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))
  dataset = dataset.batch(batch_size).prefetch(1)
  return dataset

train_dataset = windowed_dataset(dataset_10, window_size, batch_size, shuffle_buffer_size).repeat()       #repat() if the neural net ran out of series it can reusethem

model = tf.keras.models.Sequential([
                                    
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True,input_shape=(None,1000))),    #input_shape = (None, numer of multivariants)
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),
    tf.keras.layers.Dense(1000) ,                                               #Dense number are also equal to multivariants
    tf.keras.layers.Lambda(lambda x: x * 500.0)                           #multiplie output by 100 be for computing the mae

])

#model.summary()

lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))

optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),optimizer=optimizer,metrics=["mae"])

history = model.fit(train_dataset, epochs=140,steps_per_epoch=10, callbacks=[lr_schedule])       #steps_per_epoch= num_samples / batch_size  1000/100

#plot loss vs linear rate
plt.semilogx(history.history["lr"], history.history["loss"])
#plt.axis([1e-8, 1e-1, 0, 100])

tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)

tf.keras.backend.clear_session()
train_dataset = windowed_dataset(dataset_10, window_size, batch_size, shuffle_buffer_size).repeat()
model = tf.keras.models.Sequential([
                                    
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True,input_shape=(None,4500))),    #input_shape = (None, numer of multivariants)
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),  
    tf.keras.layers.Dense(1000) ,                                               #Dense number are also equal to multivariants
    tf.keras.layers.Lambda(lambda x: x * 500.0)                                         #Dense number are also equal to multivariants
                                                                 #multiplie output by 100 be for computing the mae

])

model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.SGD(lr=1e-2, momentum=0.9),metrics=["mae"])
history = model.fit(train_dataset,epochs=500,steps_per_epoch=10,verbose=2)

train_dataset = windowed_dataset(dataset_10, window_size, batch_size, shuffle_buffer_size)
h=[]
h=model.predict(train_dataset)
h.shape

time = []
for i in range(0,550):
    time.append(i)

plt.plot(time,dataset[:,75])
plt.plot(time[60:550],h[:,75])
plt.axis([100, 550, 0, 200])